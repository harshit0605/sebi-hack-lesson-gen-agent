{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0446bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv add transformers faiss-cpu torch sentence-transformers PIL opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e911c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "image_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380239ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_texts = [\"A cat sitting on a table\", \"A dog playing in the park\", \"A red sports car\", \"A bowl of fresh fruit\"]\n",
    "dataset_images = [\"cat.jpg\", \"dog.jpg\", \"car.jpg\", \"fruit.jpg\"] \n",
    "\n",
    "text_embeddings = text_model.encode(dataset_texts, convert_to_tensor=True)\n",
    "\n",
    "image_embeddings = []\n",
    "for img_path in dataset_images:\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    img_emb = image_model.generate(**inputs)\n",
    "    image_embeddings.append(img_emb)\n",
    "\n",
    "image_embeddings = torch.cat(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404084f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings = torch.cat((text_embeddings, image_embeddings)).detach().numpy()\n",
    "\n",
    "index = faiss.IndexFlatL2(data_embeddings.shape[1])\n",
    "index.add(data_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf14796",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"A cute kitten\"\n",
    "query_embedding = text_model.encode([query_text], convert_to_tensor=True).detach().numpy()\n",
    "\n",
    "distances, indices = index.search(query_embedding, k=3)\n",
    "\n",
    "print(\"Top 3 nearest MultiModal results:\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c75dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80177604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db2af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acefcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa469f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
